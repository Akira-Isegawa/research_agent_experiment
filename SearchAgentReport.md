# DeepResearch風サーチエージェントを自作して分かった、評価とファクトチェックの落とし穴

本記事は、OpenAI Agent SDK を使って「ChatGPT DeepResearch っぽい」サーチエージェントを自作し、
ワンショット検索とエージェンティック検索を比較しながら、**品質評価**と**ファクトチェック**でハマった点と対策をまとめた実験レポートです。

コード・出力・対応履歴はこのリポジトリ内の以下を参照してください。

- 実験用コード概要: `README.md`
- 実験結果（出力）: `outputs/ai_agent_research/`
- 対応履歴: `History.md`

---

## TL;DR

- エージェンティックにすると「量」と「構造」は伸びやすいが、**ハルシネーションURLが混ざる確率も上がる**
- 「評価（スコアリング）」が高くても、**根拠が怪しければ意思決定には使えない**
- ファクトチェックは「URLを差し替える係」ではなく、**合格/不合格でフィルタする係**にしないと破綻しやすい
- だから、(1) **生データ保存**、(2) **FCはフィルタ専任**、(3) **FC失敗データはレポートに入れない**が重要

---

## 背景: なぜ作ったか

エージェント的な反復（計画→調査→評価→再調査）で、
ワンショットのプロンプトエンジニアリングやコンテキストエンジニアリングより成果が出るのかを検証したかったのが動機です。

一方で、実際に動かしてみると「それっぽいレポート」は出るが、
**根拠URLが架空**だったり、**評価が甘くて早期終了**したりと、実運用にそのまま持ち込むのは危険だと感じました。

このプロジェクトでは、そのギャップを埋めるために、

- 反復型のワークフロー
- 6軸の評価
- ファクトチェッカーの導入
- 監査用ログ（生データ）保存

を段階的に追加しています。

---

## 実験システムの全体像

本リポジトリには複数の実験（`board_meeting/`, `multi_persona_hearing/`, `idea_generator_evaluator/`）がありますが、
本記事は **リサーチエージェント（このリポジトリのルート）** にフォーカスします。

本リポジトリは、3フェーズ構成です（設計意図は `README.md` 参照）。

### フェーズA: ワンショット検索

- 1回の WebSearch で発見事項と根拠情報を集める

### フェーズB: エージェンティック検索

- 調査計画を立てる
- 計画に沿って調査する（WebSearch込み）
- 評価者が 6軸で採点し、改善点があれば計画を修正
- 最大 `max_iterations` 回まで反復

### フェーズC: 比較分析

- A と B を 6軸で比較し、改善率や使い分けをレポート化

---

## 6軸評価（この実験で「品質」をどう測ったか）

評価者は、各反復の出力に対し、以下の 6軸（各0〜10点、合計60点）で採点します。

1. 目的達成度
2. 網羅性
3. 深さ・洞察力
4. 実用性
5. 信頼性・根拠性
6. 定量性・具体性

ここで重要なのは、**信頼性（根拠）が落ちると、他が良くても実用価値が下がる**という点です。

実験中、まさにここで「見た目は良いが根拠が怪しい」出力が頻発しました。

---

## 実験結果（2026-02-11の実行例）

出力フォルダ: `outputs/ai_agent_research/`

- `simple_search_20260211_000611.md`
- `agentic_search_20260211_000611.md`
- `comparison_20260211_000611.md`
- `raw_research_20260211_000611.json`

### ワンショット検索（フェーズA）

ワンショット側は、10件の発見事項・10件の根拠情報を提示しています。

- 発見事項数: 10
- 根拠情報数: 10

（詳細は `simple_search_20260211_000611.md`）

### エージェンティック検索（フェーズB）

エージェンティック側は反復しながら情報を増やしますが、
同時に「根拠として弱い（または検証不能）」なものも混ざりやすく、ファクトチェックで除外が発生しています。

`agentic_search_20260211_000611.md` の「調査プロセスの記録」より:

- 総反復回数: 5
- 最終総合スコア: 36/60
- 発見事項数: 11
- 根拠情報数: 13
- ファクトチェック検証済み（累計）: 15
- ファクトチェック除外（累計）: 12

ファクトチェック履歴（反復ごとの信頼性揺れ）:

| 反復 | 検証済み | 除外 | 信頼性 |
|---:|---:|---:|---:|
| 1 | 4 | 0 | 100.0% |
| 2 | 6 | 0 | 100.0% |
| 3 | 2 | 5 | 29.0% |
| 4 | 3 | 2 | 60.0% |
| 5 | 0 | 5 | 0.0% |

ここから分かるのは、

- **反復すればするほど常に改善するわけではない**
- ある反復で「根拠が崩れる」と、次の反復の材料としても危うくなる

という点です。

なお、この実行では、除外理由として「参照先が検索ツールから内容確認できない」「URLが実在しない」などが出ています（`agentic_search_20260211_000611.md` の詳細セクション参照）。

### 比較分析（フェーズC）

比較レポート `comparison_20260211_000611.md` は、6軸でエージェント側が改善したという整理になっています。

ただし、ここは注意点があります。

- 比較レポートは「比較分析エージェント」の生成物であり、**評価・比較そのものもLLM出力**
- 一方で、エージェンティック側の「調査プロセスの記録」には最終スコア 36/60 とある

つまり、**評価（スコア）を最終判断として鵜呑みにすると危険**です。
評価はあくまで補助で、根拠ログ（rawやFCの履歴）と一緒に監査する必要があります。

---

## 重大インシデント: 反復2〜5が全滅（URLが総ハルシネーション化）

実験の途中で、より深刻な事故が起きました（詳細は `History.md`）。

- 反復2〜5の発見事項とURLがほぼすべて架空
- 最終レポートが「それっぽいが根拠が全部偽物」という状態

この時、ワンショット（反復1相当）は比較的まともなのに、
反復を回すほど破綻する、という最悪パターンに入りました。

### 原因（要約）

`History.md` で整理した通り、主に3点です。

1. **FCが代替URLを探して差し替える**設計になっていた
2. 実装側で、FC出力が `current_result` を**上書き**してしまい、researcherの生データが失われた
3. FCが失敗した反復でも、データがレポートに混入した

つまり、ファクトチェッカーが「検証者」ではなく「生成者」になった瞬間に壊れました。

---

## 対策: FCを“合格/不合格フィルタ”にし、生データを保存する

今回の修正でやったこと（詳細は `History.md` と該当コード参照）:

### 1) researcherの生出力をそのまま保存

- 各反復の `findings` / `evidence` を一切変更せず `raw_results` に保存
- `raw_research_{timestamp}.json` として出力
- レポート（`agentic_search_*.md`）にも付録として出力

狙い: あとから「どこで壊れたか」を追跡できるようにする。

### 2) FCはURLを変更しない

- FCの指示から「代替URL探索」「replaced/unverified」概念を削除
- verified / fabricated の2値で **振り分けるだけ**にする

狙い: LLMにURL生成をさせない。

### 3) FC失敗データはレポートに入れない

- FCを通過したものだけを `accepted_findings` / `accepted_evidence` に蓄積
- 最終レポートは accepted のみを採用

狙い: 「根拠が怪しいが文章は良い」データを混入させない。

---

## 実験の所感（正直な感想）

リポジトリの `README.md` に書いた所感を、この実験の文脈に寄せて要約します。

- 量は増えるし、AI評価では改善しているように見える
- ただ、プロンプト/コンテキストの工夫でも同程度までいける可能性があり、差分の立証が難しい
- 反復を回すほどハルシネーションが増え、根拠が崩れると価値が落ちる
- “批判的な問い”が出にくく、意思決定を委ねるのは危険（＝評価の甘さと相性が悪い）
- DeepResearchの品質は、改めて「よくできている」と実感した

個人的な結論としては、
**「反復さえ回せば良くなる」ではなく、監査（根拠ログ）とガードレール（FCフィルタ）が無いと破綻しやすい**です。

---

## 再現手順（最低限）

環境構築は `requirements.txt` と `README.md` を参照。

```bash
pip install -r requirements.txt
export OPENAI_API_KEY=sk-...

# 実行（例）
python main.py "AIエージェントを企画やアイデア出しに用いる研究について調査をしてください" \
  --max-iterations 5 \
  --output-dir outputs/ai_agent_research
```

出力は `outputs/...` に `simple_search_*.md`, `agentic_search_*.md`, `comparison_*.md`, `raw_research_*.json` が生成されます。

---

## 学び（ベストプラクティス）

最後に、今回の実験から得た「壊れやすいポイント」と「守り方」を短くまとめます。

1. **URLをLLMに生成させない**（特に“代替URL”は危険）
2. **検証者（FC）はフィルタに徹する**（加工・修正をさせない）
3. **生ログ（raw）を残す**（監査できない出力は使えない）
4. **評価（スコア）より根拠を優先**（スコアは補助）
5. **反復は万能ではない**（反復で悪化するケースが普通にある）

---

## 参考リンク（リポジトリ内）

- `README.md`
- `History.md`
- `README.md`
- `workflows/research.py`
- `agent_definitions/fact_checker.py`
- `outputs/ai_agent_research/`
